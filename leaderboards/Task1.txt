{"data": [{"BD": 0.852, "mAP": 0.705, "SP": 0.632, "description": "We adopt two pipelines for this challenge. And the final result is a fusion version, simply by NMS, of these two pipelines. One of the pipelines is based on MASK RCNN (https://arxiv.org/abs/1703.06870). We modified the architecture of the network for training, to be exact, we adopt RPN-FRCN-Mask concatenated architecture, instead of \u2018Fast RCNN head\u2019 & \u2018Mask head\u2019 paralleling structure. And we augmented the network as PANet does (https://arxiv.org/abs/1803.01534). The other pipeline\u2019s network is same as before, but we remove \u2018Mask head\u2019 and use SLPR (https://arxiv.org/abs/1801.09969) to fit the outline of the object, then we use object\u2019s outline to generate a oriented rectangle for RoIRotate, so we adopt Cascade R-CNN as (https://arxiv.org/abs/1712.00726) with two steps, the first step we propose a horizontal rectangle, the second step we propose a oriented rectangle with SLPR, finally, we regress four quadrilateral vertices. Besides, we use some tricks for better performance, including class balance resampling, image rotation, multi-scale training & testing, model assembling. When model assembling, we use ResNet-50, ResNet101 and ResNeXt-101 as backbone network, respectively. Finally, we combine training set with validation set for training.", "GTF": 0.686, "Bridge": 0.43, "TeamNames": "USTC-NELSLIP", "BC": 0.843, "SV": 0.74, "ST": 0.761, "LV": 0.768, "Harbor": 0.556, "Plane": 0.902, "SBF": 0.639, "RA": 0.495, "HC": 0.639, "Ship": 0.731, "TC": 0.9, "TeamMembers": "Yixing Zhu, Chixiang Ma, Jun Du"}, {"BD": 0.677, "mAP": 0.622, "SP": 0.541, "description": "We used RBox-CNN proposed by ours. RBox-CNN regress Rotated Bounding Box(RBox) based on Faster R-CNN. The framework of RBox-CNN is similar to RRPN[Ma, Jianqi, et al]. In other words, RBox-CNN used rotation anchors and Rotation RoI pooling layer(RRoI). RBox is defined as (x, y, w, h, a), where (x, y) denotes the position of centers, w, h are the width and height, a is an angle of the RBox, respectively. The regression for RBox similar to RR-CNN[Zikun Liu, et al]. \nTo detect objects of DOTA, the backbone of RBox-CNN is ResNet101. Images are patched in similar to DOTA[Gui-Song Xia et al.]. However, patch size is 768 x 768 pixels and a stride set to 384 due to memory limitations. The location of an object in DOTA is changed from a quadrilateral bounding box to RBox. To train the model, We used the train set for 300k iterations.", "GTF": 0.512, "Bridge": 0.457, "TeamNames": "jmkoo", "BC": 0.675, "SV": 0.705, "ST": 0.669, "LV": 0.684, "Harbor": 0.556, "Plane": 0.876, "SBF": 0.4, "RA": 0.434, "HC": 0.534, "Ship": 0.707, "TC": 0.904, "TeamMembers": "Jamyoung Koo"}, {"BD": 0.754, "mAP": 0.598, "SP": 0.441, "description": "1. We use Detectron to conduct our experiments. Detectron is Facebook AI Research's software system that implements state-of-the-art object detection algorithms. We use Faster-RCNN as our base algorithm. The RPN(Region Proposal Network) and RoIs(Region-of-Interest) feature are adapted by replacing the single-scale feature map with FPN(Feature Pyramid Network). For the prediction of OBB(Oriented Bounding Box) which is better for ODAI contest, we modify its bounding boxes regression part to regress oriented bounding boxes.We use the pre-trained ResNet-50 and ResNet-101 models available in the Detectron Model Zoo for our experiments.\n2. Aerial images are usually very large in size compared to those in natural images dataset. The original size of images in ODAI dataset ranges from about 800*800 to about 4000*4000. Feeding original images into the CNN network is obviously not a good idea. Therefore, we use Random Crop method during training to randomly select an S*S image patch from original input image and S is also randomly selected from [512, 1024, 1536, 2048]. This ensures that input image patches contain multi-scale information which is beneficial for network training. During cropping we need to ensure that the image patch contains at least one object and objects that are on the edge should be filtered properly. The strategy is to keep objects that more than half of the area is inside the patch, and filter others. We try cropping several times until we get the image patch that meets the requirements, and we will return the original image to prevent the program from going into an endless loop, if we can't get a proper patch after trying 50 times. When we get the four-point representation of the labels, we should sort the vertices properly if we want to predict them precisely. \n3. In the testing phase, first we send the cropped image patches (size 1024*1024, stride 512) to obtain temporary results and then combine the results together through NMS(Non-Maximum Suppression) to restore the detecting results on the original image.", "GTF": 0.629, "Bridge": 0.347, "TeamNames": "HUST_MCLAB", "BC": 0.698, "SV": 0.585, "ST": 0.688, "LV": 0.633, "Harbor": 0.471, "Plane": 0.887, "SBF": 0.508, "RA": 0.391, "HC": 0.405, "Ship": 0.635, "TC": 0.901, "TeamMembers": "Xudong Rao, Xinggang Wang, Wenyu Liu"}, {"BD": 0.68, "mAP": 0.578, "SP": 0.393, "description": "Description of Task1\nOur method densely predicts the oriented objects. We train the FCN-\nbased model to generate score maps, geometry maps and angle maps\nsimultaneously, and the structure of our detection model is illustrated in\nfigure 1. The output layer contains several conv1x1 operations to project\n128 channels of feature maps into K channel of score maps, 4K channel of\ngeometry maps and K channel of angle maps, where K represents the\nnumber of object categories, 4K channels represent 4 distances from the\ndense per-pixel location to the top, right, bottom, left boundaries of the\noriented rectangles for each category.\nOur loss function is defined on dense per-pixel for each category. We\nuse the idea of focal loss for dense object detection to calculate the loss of\nscore maps and intersection-over-union (IoU) loss is used for the geometry\nmaps and the loss of rotation angle is computed as cosine loss. We utilize\nDOTA-devkit to crop the images and merge the results. In training phase,\nwe transform the quadrilateral into the oriented rectangle and train our\nmodel on training set and validation set of object detection in aerial images\n(DOTA) and no extra data is used. During training, we use over-sampling\nto deal with class imbalance and we use a weight decay of 1e-5 and a fixed\nlearning rate of 1e-3. By default, we use single-scale training, images are\nresized such that the scale is 512x512 pixels. Each GPU holds 8 images. In\ntest phase, we use the multi-scale testing images and we average the scoremaps, geometry maps and angle maps of each model by combining several\nmodels. To reduce redundancy, we adopt non-maximum suppression\n(NMS) and keep the threshold of NMS as 0.2, then remove extremely small\nobjects.", "GTF": 0.61, "Bridge": 0.224, "TeamNames": "NWPU_SAIP", "BC": 0.748, "SV": 0.602, "ST": 0.666, "LV": 0.657, "Harbor": 0.425, "Plane": 0.796, "SBF": 0.61, "RA": 0.383, "HC": 0.42, "Ship": 0.565, "TC": 0.887, "TeamMembers": "yuzhu zhang\uff0cpeng wang\uff0cying li\uff0cchunhua shen"}, {"BD": 0.449, "mAP": 0.531, "SP": 0.001, "description": "Object detection of satellite image play an important\nrole in the field of remote sensing imagery analytics. Recently,\ndeep learning methods have achieved impressive performance in\ncomputer vision tasks. However, existing object detection methods\nare not robust to remote sensing images due to poor capacity for\nrotation variation and concentrated object. In this work, we\npropose an efficient framework based on region-based\nconvolutional neural networks (R-CNNs) for arbitrary-oriented\nand multi-class object detection in remote sensing images. To\nlocate multi-angle objects accurately, we use the region proposal\nnetwork (RPN) to predict candidate bounding boxes with object\norientation information at each position. To address the multi-\nscale object detection problem, a top-down module is proposed to\nincorporate high-level contextual features into the deep\nconvolution networks at all scales to learn a multi-scale feature\nrepresentation for detection. For each object proposals, we extract\nits features from multiple layers to capture different level\ninformation. The features are fed into a sequence of fully\nconvolutional networks to predict a category-specific rotation\nbounding box for each object. We employ experiments on the\npublic dataset for object detection in aerial images. The result\ndemonstrates that our method is more robust against rotation\nobject and remote sensing image on object detection tasks", "GTF": 0.468, "Bridge": 0.43, "TeamNames": "changzhonghan", "BC": 0.659, "SV": 0.665, "ST": 0.695, "LV": 0.699, "Harbor": 0.456, "Plane": 0.802, "SBF": 0.438, "RA": 0.338, "HC": 0.274, "Ship": 0.695, "TC": 0.896, "TeamMembers": "Ding Ni, Zhonghan Chang"}, {"BD": 0.745, "mAP": 0.506, "SP": 0.387, "description": "We used the code of Faster R-CNN from the project of Deformable ConvNets. The code is modified to be adaptive to OBB. The preprocess and the post process followed the method in the DevKit. For example, the size of img is 1024 * 1024, the stride of spliting is 512 and the threshold of NMS is 0.1. We changed the params like the num of sampled rois in order to make the alg adptive to the DOTA dataset. However, the changes are time-wasted, so we didn't make a complete change. So if the server is still open after the comp, the result may be higher. Because the shape of some of the predicted bounding boxes is unormal, we removed the boxes. We found the min-area-rect of the bounding box and calculated the area. If the area of bounding box / the area of rect is less than a threshold, we remove it. Now the threshold is 0.8.", "GTF": 0.612, "Bridge": 0.159, "TeamNames": "madebyrag", "BC": 0.661, "SV": 0.343, "ST": 0.53, "LV": 0.419, "Harbor": 0.379, "Plane": 0.786, "SBF": 0.544, "RA": 0.356, "HC": 0.461, "Ship": 0.337, "TC": 0.874, "TeamMembers": "Mingtao Fu, Yongchao Xu"}, {"BD": 0.527, "mAP": 0.42, "SP": 0.324, "description": "We adopt the idea of R2CNN[1] which adds oriented bounding boxes directly to the end of RPN, And the horizontal bounding boxes regression is also kept to monitor the training of oriented bounding boxes. To facilitate prediction of different scale of objects, we adopt FPN in the RPN with dense connection according to R-DFPN[2]. We use larger NMS bboxes per image and that per class in fast-rcnn to predict densely located objects like vehicles and ships. We also use larger learning rate to accelerate the training because of the much larger proposal number we use.\n[1] Jiang Y, Zhu X, Wang X, et al. R2CNN: rotational region CNN for orientation robust scene text detection[J]. arXiv preprint arXiv:1706.09579, 2017.\n[2] Yang X, Sun H, Fu K, et al. Automatic Ship Detection in Remote Sensing Images from Google Earth of Complex Scenes Based on Multiscale Rotation Dense Feature Pyramid Networks[J]. Remote Sensing, 2018, 10(1): 132.", "GTF": 0.355, "Bridge": 0.133, "TeamNames": "mfhan", "BC": 0.439, "SV": 0.549, "ST": 0.491, "LV": 0.545, "Harbor": 0.306, "Plane": 0.752, "SBF": 0.177, "RA": 0.293, "HC": 0.194, "Ship": 0.445, "TC": 0.771, "TeamMembers": "Mingfeihan, Guiyang Liu"}]}